Stanford Machine Learning
=========================

Problem 1: Predicting Housing Prices
------------------------------------

#### Training Examples

| Size of House (ftÂ²) | # Bedrooms | $ Worth (1,000s) |
| ------------------- | ---------- | ---------------- |
| 2104                | 3          | 400              |
| 1406                | 2          | 232              |
| 1539                | 3          | 315              |
| 852                 | 2          | 178              |
| 1440                | 4          | 240              |

```
m = # training examples
x = "input" variables / features
y = "output" variable / "target" variable
(x, y) = training example
(x^i, y^i) = i^th training example   (aka the specific row, where _i_ represents the index into that certain row)


                          .----------------.
                         | Trainig Examples |
                          '----------------'
                                  â†“
                         .------------------.
                        | Learning Algorithm |
                         '------------------'
                                  â†“
                       .---------------------.
Ex: New Living Area â†’ | h (hypothosis/output) |  â†’ Estimated Price
         (x)           '---------------------'          (y)
```

#### Linear Representation  
```
h(x) = ğš¹â‚€ + ğš¹â‚x

n  = # features
ğš¹  = parameters

xâ‚€ = 1
xâ‚ = Size ftÂ²
xâ‚‚ = # bedrooms

h(x) = hâ‚”(x) = ğš¹â‚€ + ğš¹â‚xâ‚ + ğš¹â‚‚xâ‚‚    <- supposed to be `h` subscript theta of `x`
       n
h(x) = Î£ ğš¹áµ¢xáµ¢ = (ğš¹^t)x             <- sum from 0 to `n`.  Theta to the t times x is `transpose`
      i=0
         n
j(ğš¹) = Â½ Î£ (hğš¹                     <- j(ğš¹) basically represents the "y" axis ***COME BACK TO THIS
        i=1
```


#### Gradient Descent  
(Stanford Lecture 2: roughly 33 min in)  
```
ğš¹ = 0
ğš¹áµ¢ := ğš¹áµ¢ + ğ›‚(2/ğš¹áµ¢)j(ğš¹)             <- the `:=` notation is just overwriting the variable

... add some calculations

ğš¹áµ¢ := ğš¹áµ¢ - ğ›‚(h(x) - y)yáµ¢           <- ğ›‚ = paramater called the `learning rate`
```
